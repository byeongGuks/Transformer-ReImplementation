{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DMIS\\AppData\\Roaming\\Python\\Python39\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dataset(\"iwslt2017\", 'iwslt2017-de-en', cache_dir='./data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " video: ♪♫ frosty the coal man is a jolly, happy soul.    \n",
      " curious historical footnote:  when the moors invaded southern spain, they took this custom with them  and the pronunciation changed over the centuries  from \"allah, allah, allah,\" to \"olé, olé, olé,\"  which you still hear in bullfights and in flamenco dances.    \n",
      " in spain, when a performer has done something impossible and magic,  \"allah, olé, olé, allah, magnificent, bravo,\"  incomprehensible, there it is -- a glimpse of god.    \n",
      " if the divine, cockeyed genius assigned to your case  decides to let some sort of wonderment be glimpsed, for just one moment  through your efforts, then \"olé!\"    \n",
      " and \"olé!\" to you, nonetheless.    \n",
      " \"olé!\" to you, nonetheless,  just for having the sheer human love and stubbornness  to keep showing up.    \n",
      "   june cohen: olé!    \n",
      " my favorite is the middle one --  the mp3 player, nose hair trimmer, and crème brûlée torch.    \n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "def make_dataset(file_path, docs) :\n",
    "    f = open(file_path, \"w\")\n",
    "    for doc in docs :\n",
    "        segs = doc.findall('seg')\n",
    "        for seg in segs :\n",
    "            text = seg.text.lower()\n",
    "            try :\n",
    "                f.write(text)\n",
    "                f.write('\\n')\n",
    "            except : \n",
    "                print(text)\n",
    "    f.close()\n",
    "    \n",
    "train_folder_path = \"./data/de-en/training_and_development/\"\n",
    "test_folder_path = \"./data/de-en/test/\"\n",
    "\n",
    "## train english\n",
    "tree_en = ET.parse(train_folder_path + 'IWSLT17.TED.dev2010.de-en.en.xml')\n",
    "root_en = tree_en.getroot()\n",
    "refset = root_en.find('refset')\n",
    "docs = refset.findall('doc')\n",
    "make_dataset(\"./data/de-en/train.en\", docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract text from XML Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "def make_dataset(file_path, docs) :\n",
    "    f = open(file_path, \"w\", encoding='utf-8')\n",
    "    for doc in docs :\n",
    "        segs = doc.findall('seg')\n",
    "        for seg in segs :\n",
    "            text = seg.text.lower()\n",
    "            try :\n",
    "                f.write(text)\n",
    "                f.write('\\n')\n",
    "            except : \n",
    "                print(text)\n",
    "    f.close()\n",
    "\n",
    "train_folder_path = \"./data/de-en/training_and_development/\"\n",
    "test_folder_path = \"./data/de-en/test/\"\n",
    "\n",
    "## train english\n",
    "tree_en = ET.parse(train_folder_path + 'IWSLT17.TED.dev2010.de-en.en.xml')\n",
    "root_en = tree_en.getroot()\n",
    "refset = root_en.find('refset')\n",
    "docs = refset.findall('doc')\n",
    "make_dataset(\"./data/de-en/train.en\", docs)\n",
    "\n",
    "## train deutsch\n",
    "tree_de = ET.parse(train_folder_path + \"IWSLT17.TED.dev2010.de-en.de.xml\")\n",
    "root_de = tree_de.getroot()\n",
    "srcset = root_de.find('srcset')\n",
    "docs = srcset.findall('doc')\n",
    "make_dataset(\"./data/de-en/train.de\", docs)\n",
    "\n",
    "## test en\n",
    "tree_en_test = ET.parse(test_folder_path + 'IWSLT17.TED.tst2017.mltlng.en-de.en.xml')\n",
    "root_en_test = tree_en_test.getroot()\n",
    "srcset = root_en_test.find('srcset')\n",
    "docs = srcset.findall('doc')\n",
    "make_dataset(\"./data/de-en/test.en\", docs)\n",
    "\n",
    "## test deutsch\n",
    "tree_de_test = ET.parse(test_folder_path + 'IWSLT17.TED.tst2017.mltlng.de-en.de.xml')\n",
    "root_de_test = tree_de_test.getroot()\n",
    "srcset = root_de_test.find('srcset')\n",
    "docs = srcset.findall('doc')\n",
    "make_dataset(\"./data/de-en/test.de\", docs)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "train_folder_path = \"./data/de-en/training_and_development/\"\n",
    "test_folder_path = \"./data/de-en/test/\"\n",
    "\n",
    "## train english\n",
    "tree_en = ET.parse(train_folder_path + 'IWSLT17.TED.dev2010.de-en.en.xml')\n",
    "root_en = tree_en.getroot()\n",
    "refset = root_en.find('refset')\n",
    "docs = refset.findall('doc')\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Vocabulary : BPE Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  letztes jahr habe ich diese beiden folien gezeigt, um zu veranschaulichen, dass die arktische eiskappe, die für annähernd drei millionen jahre die grösse der unteren 48 staaten hatte, um 40 prozent geschrumpft ist.  \n",
      "\n",
      "\n",
      "\n",
      "letztes\n",
      "jahr\n",
      "habe\n",
      "ich\n",
      "diese\n",
      "beiden\n",
      "folien\n",
      "gezeigt\n",
      "um\n",
      "zu\n",
      "veranschaulichen\n",
      "dass\n",
      "die\n",
      "arktische\n",
      "eiskappe\n",
      "die\n",
      "für\n",
      "annähernd\n",
      "drei\n",
      "millionen\n",
      "jahre\n",
      "die\n",
      "grösse\n",
      "der\n",
      "unteren\n",
      "48\n",
      "staaten\n",
      "hatte\n",
      "um\n",
      "40\n",
      "prozent\n",
      "geschrumpft\n",
      "ist.\n",
      "\n",
      "\n",
      "{'</w>': 14, 'l': 2, 'e': 2, 't': 9, 'z': 4, 's': 9, 'j': 2, 'a': 12, 'h': 5, 'r': 8, 'b': 2, 'i': 4, 'c': 0, 'd': 5, 'n': 6, 'f': 3, 'o': 3, 'g': 4, 'u': 6, 'm': 4, 'v': 1, 'k': 2, 'p': 4, 'ü': 1, 'ä': 1, 'ö': 1, '4': 2, '8': 1, '0': 1, '.': 1, 'e</w>': 10, 'en': 1, 'en</w>': 6, 'ch': 5, 'di': 4, 'ei': 4, 'er': 4, 'es': 3, 'li': 3, 't</w>': 3}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "## return (maximum counted pair, count)\n",
    "## ex - (h st, 10)\n",
    "def search_max_pair(dict) :\n",
    "    pair_count = {}\n",
    "    \n",
    "    ## pair count\n",
    "    for word in dict:\n",
    "        unit_list = word.split(' ')\n",
    "        count_of_word = dict[word] \n",
    "        #print(unit_list)\n",
    "        for i in range(len(unit_list) - 1):\n",
    "            new_word = unit_list[i] + ' ' + unit_list[i+1] ## insert space for distinguish frontword and backword\n",
    "            pair_count[new_word] = pair_count[new_word] + count_of_word if (new_word in pair_count) else count_of_word\n",
    "    ## search maximum pair\n",
    "    max_count = 0\n",
    "    max_pair = ''\n",
    "    for pair in pair_count:\n",
    "        if pair_count[pair] > max_count :\n",
    "            max_count = pair_count[pair]\n",
    "            max_pair = pair\n",
    "    return (max_pair, max_count)\n",
    "\n",
    "def merge_word_dict(dict, frontword, backword) :\n",
    "    new_word_dict = {}\n",
    "    for word in dict :\n",
    "        unit_list = word.split(' ')\n",
    "        new_word = ''\n",
    "        i=0\n",
    "        while i < len(unit_list) :\n",
    "            if i == len(unit_list)-1 or unit_list[i] != frontword or unit_list[i+1] != backword : \n",
    "                new_word = new_word + unit_list[i] + ' '\n",
    "            else :\n",
    "                new_word = new_word + unit_list[i] + unit_list[i+1] + ' '\n",
    "                i += 1 ## skip next unit\n",
    "            i += 1\n",
    "        new_word = new_word[:-1]\n",
    "        new_word_dict[new_word] = dict[word] ## change word with new key\n",
    "    return new_word_dict\n",
    "                \n",
    "                \n",
    "        \n",
    "\n",
    "def byte_pair_encoding(file_path, count=10) : \n",
    "    f = open(file_path, \"r\", encoding='utf-8')\n",
    "    stop_word_set = set([',', ' ', '-', '', '\\n', '.', '!'])\n",
    "    word_dict = {}  ## key-bpe encoded words / value-count : word set for calculate maximun counted pair\n",
    "    vocabulary = {} ## key-vacab word / value-count : vocabulary for encoding, \n",
    "    vocabulary['</w>'] = 0\n",
    "    while True :\n",
    "        line = f.readline()\n",
    "        if not line :\n",
    "            break\n",
    "        words = line.split(' ')\n",
    "        for word in words :\n",
    "            word = re.sub('[,\\-\\n!]', '', word)\n",
    "            print(word)\n",
    "            if word not in stop_word_set :\n",
    "                splited_word = ''\n",
    "                for character in [*word]:\n",
    "                    splited_word = splited_word + character + ' '\n",
    "                    vocabulary[character] = vocabulary[character] + 1 if (character in vocabulary) else 1\n",
    "                splited_word = splited_word + '</w>'\n",
    "                word_dict[splited_word] = word_dict[splited_word] + 1 if (splited_word in word_dict) else 1\n",
    "                vocabulary['</w>'] += 1\n",
    "\n",
    "    for i in range(count) :\n",
    "        max_pair, max_count = search_max_pair(word_dict)\n",
    "        subwords = max_pair.split(' ')\n",
    "        frontword = subwords[0]\n",
    "        backword = subwords[1]\n",
    "        \n",
    "        word_dict = merge_word_dict(word_dict, frontword=frontword, backword=backword)\n",
    "        vocabulary[frontword + backword] = max_count\n",
    "        vocabulary[frontword] = vocabulary[frontword] - max_count\n",
    "        vocabulary[backword] = vocabulary[backword] - max_count\n",
    "    print(vocabulary)\n",
    "        \n",
    "        \n",
    "file_path = 'C://Users/DMIS/project/transformer/data/de-en/train.de'\n",
    "#file_path = 'C://Users/DMIS/project/transformer/data/de-en/bpe_ex.en' ## for test\n",
    "\n",
    "byte_pair_encoding(file_path, 10)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m positions \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m10\u001b[39m))\n\u001b[1;32m----> 6\u001b[0m p \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m((\u001b[39mlambda\u001b[39;00m elem: np\u001b[39m.\u001b[39msin(elem\u001b[39m/\u001b[39mnp\u001b[39m.\u001b[39mpower(\u001b[39m10000\u001b[39m, elem\u001b[39m/\u001b[39m\u001b[39m512\u001b[39m))),  positions[:,\u001b[39m0\u001b[39;49m::\u001b[39m2\u001b[39;49m] ))\n\u001b[0;32m      7\u001b[0m \u001b[39mprint\u001b[39m(p)\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "positions = list(range(0, 10))\n",
    "\n",
    "\n",
    "p = list(map((lambda elem: np.sin(elem/np.power(10000, elem/512))),  positions[:,0::2] ))\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0. 1. 0. 1. 0.]\n",
      " [0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "(3, 5)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "mat = np.zeros((3, 5))\n",
    "\n",
    "print(mat)  # 3 rows and 5 columns of zeros\n",
    "mat[0:2, 1::2] = 1\n",
    "print(mat)  # all of second row is now on\n",
    "print(mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (256) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m p_encoding_value \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(sequence_length, model_dimension)\n\u001b[0;32m      4\u001b[0m positions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(model_dimension)[:, np\u001b[39m.\u001b[39mnewaxis]\n\u001b[1;32m----> 5\u001b[0m p_encoding_value[:, \u001b[39m0\u001b[39m::\u001b[39m2\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m((\u001b[39mlambda\u001b[39;00m elem: np\u001b[39m.\u001b[39msin(elem\u001b[39m/\u001b[39mnp\u001b[39m.\u001b[39mpower(\u001b[39m10000\u001b[39m, elem\u001b[39m/\u001b[39mmodel_dimension))),  positions[\u001b[39m0\u001b[39m::\u001b[39m2\u001b[39m])))\n\u001b[0;32m      6\u001b[0m p_encoding_value[:, \u001b[39m1\u001b[39m::\u001b[39m2\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m((\u001b[39mlambda\u001b[39;00m elem: np\u001b[39m.\u001b[39msin(elem\u001b[39m/\u001b[39mnp\u001b[39m.\u001b[39mpower(\u001b[39m10000\u001b[39m, (elem\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m/\u001b[39mmodel_dimension))),  positions[\u001b[39m0\u001b[39m::\u001b[39m2\u001b[39m])))\n\u001b[0;32m      9\u001b[0m \u001b[39mprint\u001b[39m(p_encoding_value)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (256) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "sequence_length = 10\n",
    "model_dimension = 512\n",
    "p_encoding_value = torch.zeros(sequence_length, model_dimension)\n",
    "positions = np.arange(model_dimension)[:, np.newaxis]\n",
    "p_encoding_value[:, 0::2] += torch.tensor(list(map((lambda elem: np.sin(elem/np.power(10000, elem/model_dimension))),  positions[0::2])))\n",
    "p_encoding_value[:, 1::2] += torch.tensor(list(map((lambda elem: np.sin(elem/np.power(10000, (elem-1)/model_dimension))),  positions[0::2])))\n",
    "\n",
    "\n",
    "print(p_encoding_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 512)\n"
     ]
    }
   ],
   "source": [
    "model_dimension = 512\n",
    "sequence_length = 10\n",
    "\n",
    "def __make_positional_vector (pos, model_dimension) :\n",
    "        return [pos/np.power(10000, 2*(hidden_i//2)/ model_dimension) for hidden_i in range(model_dimension)]\n",
    "\n",
    "positional_encodings = np.array([__make_positional_vector(i, model_dimension) for i in range(sequence_length)])\n",
    "positional_encodings[:, 0::2] = np.sin(positional_encodings[:, 0::2])\n",
    "positional_encodings[:, 1::2] = np.cos(positional_encodings[:, 1::2])\n",
    "\n",
    "print(positional_encodings.shape)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14 14]\n",
      " [14 14]]\n"
     ]
    }
   ],
   "source": [
    "m1 = np.array([[1,1],\n",
    "          [2,2],\n",
    "          [3,3]])\n",
    "print(np.dot(m1, m1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
